{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "# 设定神经网络的超参数\n",
    "# 定义神经网络可以接收的图片的尺寸和通道数\n",
    "IMAGE_SIZE_W = 800\n",
    "IMAGE_SIZE_H = 352\n",
    "NUM_CHANNELS = 3\n",
    "# 定义第一层卷积核的大小和个数，same卷积，后跟池化4X4并且4步\n",
    "CONV1_SIZE = 3\n",
    "CONV1_KERNEL_NUM = 64\n",
    "# 定义第二层卷积核的大小和个数，same卷积,后跟池化2X2并且2步\n",
    "CONV2_SIZE = 3\n",
    "CONV2_KERNEL_NUM = 128\n",
    "# 定义第3层卷积核的大小和个数,sanme卷积，后跟池化2X2并且2步\n",
    "CONV3_SIZE = 3\n",
    "CONV3_KERNEL_NUM = 256\n",
    "# 定义第4层卷积核的大小和个数，same卷积，后跟池化2X2并且2步\n",
    "CONV4_SIZE = 3\n",
    "CONV4_KERNEL_NUM = 256\n",
    "#删除所有的全连接层，参数量过于庞大\n",
    "\n",
    "# 定义第5层全连接层的神经元个数\n",
    "#FC_SIZE = 60000\n",
    "# 定义第6层全连接层的神经元个数\n",
    "#OUTPUT_NODE = 57600\n",
    "# 定义初始化网络权重函数\n",
    "\n",
    "\n",
    "#输出的两个不同大小\n",
    "SIZE_1 = [176,400]\n",
    "#resize之后的大小，也是标定的大小，resize之后跟着的四个卷积，其大小都不变跟conv4一样的same卷积。\n",
    "SIZE_2 = [120,480]\n",
    "\n",
    "#定义第二个网络的基本参数，输入是不变的\n",
    "# 定义第一层卷积核的大小和个数，2步\n",
    "S_CONV1_SIZE = 2      \n",
    "S_CONV1_KERNEL_NUM = 1\n",
    "# resize之后，定义后面四层卷积核的大小和个数\n",
    "S_CONV2_SIZE = 3\n",
    "S_CONV2_KERNEL_NUM = 1\n",
    "\n",
    "short_cut1 = np.zeros([20,176,400])\n",
    "short_cut2 = np.zeros([20,120,480])\n",
    "\n",
    "'''temp = []\n",
    "real_temp = np.empty(shape=[20,176,400])\n",
    "middle = np.empty(shape=[20,176,400])\n",
    "mid_1 = np.empty(shape=[176,400])\n",
    "\n",
    "\n",
    "y = []\n",
    "real_y = np.empty(shape=[20,120,480])\n",
    "middle_2 = np.empty(shape=[20,176,400])\n",
    "mid_2 = np.empty(shape=[176,400])'''\n",
    "\n",
    "\n",
    "\n",
    "def conv2d(x,w):\n",
    "    '''\n",
    "    args:\n",
    "    x: 一个输入 batch\n",
    "    w: 卷积层的权重\n",
    "    '''\n",
    "    # strides 表示卷积核在不同维度上的移动步长为 1,第一维和第四维一定是 1,这是因为卷积层的步长只对矩阵的长和宽有效;\n",
    "    # padding='SAME'表示使用全 0 填充,而'VALID'表示不填充\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def V_conv2d(x,w):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 2, 2, 1], padding='VALID')\n",
    "   # 定义最大池化操作函数\n",
    "def max_pool_2x2(x):\n",
    "    '''\n",
    "    args:\n",
    "    x: 一个输入 batch\n",
    "    '''\n",
    "    # ksize 表示池化过滤器的边长为 2,strides 表示过滤器移动步长是 2,'SAME'提供使用全 0 填充    \n",
    "    #return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "#定义前向传播的过程\n",
    "\n",
    "def max_pool_4x4(x):\n",
    "    #return tf.nn.max_pool(x, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "    return tf.nn.max_pool(x, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='VALID')\n",
    "def get_weight(shape, regularizer):\n",
    "    '''\n",
    "    args:\n",
    "    shape:生成张量的维度regularizer: 正则化项的权重\n",
    "    '''\n",
    "    # tf.truncated_normal 生成去掉过大偏离点的正态分布随机数的张量,stddev 是指定标准差\n",
    "    #w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    w = tf.Variable(tf.random.truncated_normal(shape,stddev=0.1))\n",
    "    # 为权重加入 L2 正则化,通过限制权重的大小,使模型不会随意拟合训练数据中的随机噪音\n",
    "    if regularizer != None: \n",
    "        #tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "        tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "    # 定义初始化偏置项函数\n",
    "def get_bias(shape):\n",
    "    '''\n",
    "    args:\n",
    "    shape:生成张量的维度\n",
    "    '''\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    # 统一将 bias 初始化为 0\n",
    "    return b\n",
    "    # 定义卷积计算函数\n",
    "def forward_1(x, train, regularizer):\n",
    "    '''\n",
    "    args:\n",
    "    x: 一个输入 batch\n",
    "    train: 用于区分训练过程 True,测试过程 False\n",
    "    regularizer:正则化项的权重\n",
    "    '''\n",
    "\n",
    "    # 实现第一层卷积层的前向传播过程\n",
    "    conv1_w = get_weight([CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_KERNEL_NUM], regularizer) # 初始化卷积核\n",
    "    conv1_b = get_bias([CONV1_KERNEL_NUM]) # 初始化偏置项\n",
    "    conv1 = conv2d(x, conv1_w) # 实现卷积运算\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b)) # 对卷积后的输出添加偏置,并过 relu 非线性激活函数\n",
    "    pool1 = max_pool_4x4(relu1) # 将激活后的输出进行最大池化\n",
    "    #print(\"------------------------------------------------------\")\n",
    "    #print(tf.shape(conv1_w ))\n",
    "\n",
    "    # 实现第二层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv2_w=get_weight([CONV2_SIZE,CONV2_SIZE,CONV1_KERNEL_NUM,CONV2_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv2 = conv2d(pool1, conv2_w) # 该层的输入就是上一层的输出 pool1\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))\n",
    "    pool2 = max_pool_2x2(relu2)\n",
    "    #print(\"------------------------------------------------------\")\n",
    "    # print(tf.shape(conv2_w))\n",
    "\n",
    "    # 实现第三层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv3_w=get_weight([CONV3_SIZE,CONV3_SIZE,CONV2_KERNEL_NUM,CONV3_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv3_b = get_bias([CONV3_KERNEL_NUM])\n",
    "    conv3 = conv2d(pool2, conv3_w) # 该层的输入就是上一层的输出 pool1\n",
    "\n",
    "    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_b))\n",
    "    pool3 = max_pool_2x2(relu3)\n",
    "\n",
    "    #print(\"------------------------------------------------------\")\n",
    "    #print(tf.shape(conv3_w))\n",
    "\n",
    "    # 实现第四层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv4_w=get_weight([CONV4_SIZE,CONV4_SIZE,CONV3_KERNEL_NUM,CONV4_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv4 = conv2d(pool3, conv4_w) # 该层的输入就是上一层的输出 pool1\n",
    "    conv4_b = get_bias([CONV4_KERNEL_NUM])\n",
    "    relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_b))\n",
    "\n",
    "    pool4 = max_pool_2x2(relu4)    \n",
    "    pool4_shape = pool4.get_shape().as_list()\n",
    "\n",
    "    '''with tf.compat.v1.Session() as sess:\n",
    "        middle = pool4.eval(session=sess)\n",
    "        for i in range(0,pool4_shape[0]):\n",
    "            mid_1 = cv.resize(middle(i),(176,400),interpolation=cv.INTER_NEAREST)\n",
    "            temp.append(mid_1)\n",
    "    for i in range(20):\n",
    "        for j in range(176):\n",
    "            for k in range(400):\n",
    "                real_temp[i][j][k]=temp[i][j][k] '''     \n",
    "\n",
    "\n",
    "        #五六七八九层都是一样的方法,此时大小为176*400\n",
    "    conv5_w=get_weight([CONV4_SIZE,CONV4_SIZE,1,CONV4_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv5 = conv2d(pool4, conv5_w) # 该层的输入就是上一层的输出 pool1\n",
    "    conv5_b = get_bias([CONV4_KERNEL_NUM])\n",
    "    relu5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_b))\n",
    "\n",
    "    conv6_w=get_weight([CONV4_SIZE,CONV4_SIZE,CONV4_KERNEL_NUM,CONV4_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv6 = conv2d(relu5, conv6_w) # 该层的输入就是上一层的输出 pool1\n",
    "    conv6_b = get_bias([CONV4_KERNEL_NUM])\n",
    "    relu6 = tf.nn.relu(tf.nn.bias_add(conv6, conv6_b))\n",
    "\n",
    "    conv7_w=get_weight([CONV4_SIZE,CONV4_SIZE,CONV4_KERNEL_NUM,CONV4_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv7 = conv2d(relu6, conv7_w) # 该层的输入就是上一层的输出 pool1\n",
    "    conv7_b = get_bias([CONV4_KERNEL_NUM])\n",
    "    relu7 = tf.nn.relu(tf.nn.bias_add(conv7, conv7_b))\n",
    "\n",
    "    conv8_w=get_weight([CONV4_SIZE,CONV4_SIZE,CONV4_KERNEL_NUM,CONV4_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv8 = conv2d(relu7, conv8_w) # 该层的输入就是上一层的输出 pool1\n",
    "    conv8_b = get_bias([CONV4_KERNEL_NUM])\n",
    "    relu8 = tf.nn.relu(tf.nn.bias_add(conv8, conv8_b))\n",
    "    short_cut1 = relu8\n",
    "\n",
    "    conv9_w=get_weight([CONV4_SIZE,CONV4_SIZE,CONV4_KERNEL_NUM,CONV4_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv9 = conv2d(relu8, conv9_w) # 该层的输入就是上一层的输出 pool1\n",
    "    conv9_b = get_bias([CONV4_KERNEL_NUM])\n",
    "    relu9 = tf.nn.relu(tf.nn.bias_add(conv9, conv9_b))\n",
    "    short_cut2 = relu9\n",
    "\n",
    "    '''with tf.Session() as sess:\n",
    "        middle_2 = relu9.eval(session=sess)\n",
    "        for i in range(0,relu9_shape[0]):            \n",
    "            mid_2 = cv2.resize(middle[i],(120,480),interpolation=cv.INTER_NEAREST)\n",
    "            y.append(mid_2)\n",
    "    for i in range(20):\n",
    "        for j in range(120):\n",
    "            for k in range(480):\n",
    "                real_y[i][j][k]=y[i][j][k]'''       \n",
    "    return relu9\n",
    "  \n",
    "#从原图再次卷积得到语义信息\n",
    "def forward_2(x, train, regularizer):\n",
    "    '''\n",
    "    args:\n",
    "    x: 一个输入 batch\n",
    "    train: 用于区分训练过程 True,测试过程 False\n",
    "    regularizer:正则化项的权重\n",
    "    '''\n",
    "    # 实现第一层卷积层的前向传播过程\n",
    "    conv1_w = get_weight([S_CONV1_SIZE, S_CONV1_SIZE, NUM_CHANNELS, S_CONV1_KERNEL_NUM], regularizer) # 初始化卷积核\n",
    "    conv1_b = get_bias([S_CONV1_KERNEL_NUM]) # 初始化偏置项\n",
    "    conv1 = V_conv2d(x, conv1_w) # 实现卷积运算\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b)) # 对卷积后的输出添加偏置,并过 relu 非线性激活函数\n",
    "\n",
    "    # 实现第二层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv2_w=get_weight([S_CONV2_SIZE,S_CONV2_SIZE,S_CONV1_KERNEL_NUM,S_CONV2_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv2 = conv2d(relu1, conv2_w) # 该层的输入就是上一层的输出 \n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b))\n",
    "\n",
    "    # 实现第二层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv3_w=get_weight([S_CONV2_SIZE,S_CONV2_SIZE,S_CONV1_KERNEL_NUM,S_CONV2_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv3 = conv2d(relu2, conv3_w) # 该层的输入就是上一层的输出\n",
    "    relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_b))+short_cut1\n",
    "\n",
    "    # 实现第二层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv4_w=get_weight([S_CONV2_SIZE,S_CONV2_SIZE,S_CONV1_KERNEL_NUM,S_CONV2_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv4 = conv2d(relu3, conv4_w) # 该层的输入就是上一层的输出\n",
    "    relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_b))+short_cut2\n",
    "    \n",
    "    # 实现第二层卷积层的前向传播过程,并初始化卷积层的对应变量\n",
    "    conv5_w=get_weight([S_CONV2_SIZE,S_CONV2_SIZE,S_CONV1_KERNEL_NUM,S_CONV2_KERNEL_NUM],regularizer) # 该层每个卷积核的通道数要与上一层卷积核的个数一致conv2_b = get_bias([CONV2_KERNEL_NUM])\n",
    "    conv5 = conv2d(relu4, conv5_w) # 该层的输入就是上一层的输出\n",
    "    relu5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_b))\n",
    "    #pool_shape = relu5.get_shape().as_list()\n",
    "    y = []\n",
    "    real_y = np.empty(shape=[20,120,480])\n",
    "    with tf.Session() as sess:\n",
    "        middle = relu5.eval(session = sess)\n",
    "        for i in range(0,20):            \n",
    "            mid = cv2.resize(middle[i],(120,480),interpolation=cv.INTER_NEAREST)\n",
    "            y.append(mid)\n",
    "    for i in range(20):\n",
    "        for j in range(120):\n",
    "            for k in range(480):\n",
    "                real_y[i][j][k]=y[i][j][k]       \n",
    "    return real_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from scipy.io import loadmat\n",
    "# 定义训练过程中的超参数\n",
    "BATCH_SIZE = 20 # 一个 batch 的数量\n",
    "LEARNING_RATE_BASE = 0.005 # 初始学习率\n",
    "LEARNING_RATE_DECAY = 0.99 # 学习率的衰减率\n",
    "REGULARIZER = 0.0001 # 正则化项的权重\n",
    "STEPS = 5000 # 最大迭代次数\n",
    "MOVING_AVERAGE_DECAY = 0.99 # 滑动平均的衰减率\n",
    "MODEL_SAVE_PATH=\"./model/\" # 保存模型的路径\n",
    "MODEL_NAME=\"Depth_Model\" # 模型命名\n",
    "# 训练过程\n",
    "def backward(p1,p2):\n",
    "    # x, y_是定义的占位符,需要指定参数的类型,维度(要和网络的输入与输出维度一致),类似 于函数的形参,运行时必须传入值\n",
    "    #x = tf.placeholder(tf.float32,[BATCH_SIZE,forward1.IMAGE_SIZE_H,forward1.IMAGE_SIZE_W,forward1.NUM_CHANNELS])\n",
    "    x = tf.placeholder(tf.float32,[BATCH_SIZE,IMAGE_SIZE_H,IMAGE_SIZE_W,NUM_CHANNELS])\n",
    "    print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%p1\")\n",
    "    print(p1.shape)\n",
    "    print(type(p1))    \n",
    "    \n",
    "    #y_ = tf.placeholder(tf.float32, [20,120,480])\n",
    "    y_ = tf.placeholder(tf.float32, [20,120,480])\n",
    "    #y = forward1.forward_1(x,True, REGULARIZER)+forward1.forward_2(x,True,REGULARIZER) # 调用前向传播网络得到维度为  10 的 tensor\n",
    "    y = forward_1(x,True, REGULARIZER)\n",
    "    temp = np.empty(shape = [20,120,480])\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        middle = sess.run(y.eval())\n",
    "        print(type(middle))\n",
    "        for i in range(0,relu9_shape[0]):            \n",
    "            mid = cv2.resize(middle[i],(120,480),interpolation=cv.INTER_NEAREST)\n",
    "            mid_1.append(mid)\n",
    "    for i in range(20):\n",
    "        for j in range(120):\n",
    "            for k in range(480):\n",
    "                temp[i][j][k]=mid_1[i][j][k]   \n",
    "    real_y =  tf.convert_to_tensor(temp)                 \n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False) # 声明一个全局计数器,并输出化为 0\n",
    "    # 先是对网络最后一层的输出 y 做 softmax,通常是求取输出属于某一类的概率,其实就是一个 num_classes 大小的向量,\n",
    "    # 再将此向量和实际标签值做交叉熵,需要说明的是该函数返回的是一个向量\n",
    "    \n",
    "    #ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    print(type(y))\n",
    "    mse = tf.reduce_sum(tf.square(real_y-y_)) # 再对得到的向量求均值就得到 loss\n",
    "    \n",
    "    loss = mse + tf.add_n(tf.get_collection('losses')) # 添加正则化中的 losses'''     \n",
    "    \n",
    "    # 实现指数级的减小学习率,可以让模型在训练的前期快速接近较优解,又可以保证模型在训练后期不会有太大波动\n",
    "    \n",
    "    # 计算公式:decayed_learning_rate=learining_rate*decay_rate^(global_step/decay_steps)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    LEARNING_RATE_BASE,\n",
    "    global_step,\n",
    "    1,\n",
    "    LEARNING_RATE_DECAY,\n",
    "    staircase=True) # 当 staircase=True 时,(global_step/decay_steps)则被转化为整数,以此来选择不同的衰减方式\n",
    "   \n",
    "    # 传入学习率,构造一个实现梯度下降算法的优化器,再通过使用 minimize 更新存储要训练的变量的列表来减小 loss\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "   \n",
    "    # 实现滑动平均模型,参数 MOVING_AVERAGE_DECAY 用于控制模型更新的速度。训练过程中会对每一个变量维护一个影子变量,这个影子变量的初始值\n",
    "    # 就是相应变量的初始值,每次变量更新时,影子变量就会随之更新\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "   \n",
    "    with tf.control_dependencies([train_step, ema_op]): # 将 train_step 和 ema_op 两个训练操作绑 定到 train_op 上\n",
    "        train_op = tf.no_op(name='train')    \n",
    "    with tf.Session() as sess: \n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver() # 实例化一个保存和恢复变量的 saver\n",
    "        print(\"已经完成实例化\")\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "        print(\"已完成所有变量的初始化\")\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH) # 通过 checkpoint 文件定位到最新保存的模型\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path) # 加载最新的模型\n",
    "        for i in range(STEPS):\n",
    "            xs, ys = p1,p2   # 读取一个 batch 的数据,这里我直接传入一个batch,一个一个的传,也就10个batch\n",
    "            reshaped_xs = np.reshape(xs,(\n",
    "            # 将输入数据 xs 转换成与网络输入相同形状的矩阵\n",
    "            BATCH_SIZE,\n",
    "            382,\n",
    "            800,\n",
    "            forward1.NUM_CHANNELS),dtype=np.float32)\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%xs\")\n",
    "            print(xs.shape)\n",
    "            print(type(xs))\n",
    "                # 喂入训练图像和标签,开始训练\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x:xs,y_: ys})\n",
    "            if i % 100 == 0: # 每迭代 100 次打印 loss 信息,并保存最新的模型\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step,loss_value))       \n",
    "                saver.save(sess,\n",
    "                os.path.join(MODEL_SAVE_PATH,\n",
    "                MODEL_NAME),\n",
    "                global_step=global_step)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 21:28:30.257972 139675257571136 deprecation.py:323] From /home/hp209/env/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%p1\n",
      "(20, 352, 800, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_18' with dtype float and shape [20,352,800,3]\n\t [[node Placeholder_18 (defined at <ipython-input-30-eb40da026dc6>:19) ]]\n\nOriginal stack trace for 'Placeholder_18':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hp209/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2880, in _run_cell\n    return runner(coro)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3248, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-4cbc9a548466>\", line 35, in <module>\n    backward(real_data,real_label)\n  File \"<ipython-input-30-eb40da026dc6>\", line 19, in backward\n    x = tf.placeholder(tf.float32,[BATCH_SIZE,IMAGE_SIZE_H,IMAGE_SIZE_W,NUM_CHANNELS])\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6262, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_18' with dtype float and shape [20,352,800,3]\n\t [[{{node Placeholder_18}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4cbc9a548466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mreal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-eb40da026dc6>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(p1, p2)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmiddle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrelu9_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \"\"\"\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5577\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5578\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5579\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_18' with dtype float and shape [20,352,800,3]\n\t [[node Placeholder_18 (defined at <ipython-input-30-eb40da026dc6>:19) ]]\n\nOriginal stack trace for 'Placeholder_18':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hp209/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/hp209/env/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hp209/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2880, in _run_cell\n    return runner(coro)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3248, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/hp209/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-4cbc9a548466>\", line 35, in <module>\n    backward(real_data,real_label)\n  File \"<ipython-input-30-eb40da026dc6>\", line 19, in backward\n    x = tf.placeholder(tf.float32,[BATCH_SIZE,IMAGE_SIZE_H,IMAGE_SIZE_W,NUM_CHANNELS])\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6262, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/hp209/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "#print(type(label))\n",
    "label_folder = '/home/hp209/Mocular/black120480/'\n",
    "files =os.listdir(label_folder)\n",
    "files.sort()\n",
    "for file in files: \n",
    "    file_path = os.path.join(label_folder,file)\t \n",
    "    a = np.loadtxt(file_path)\n",
    "    a = np.array(a,dtype = np.float32)\n",
    "    label.append(a)   \n",
    "real_label = np.empty(shape=[20,120,480])\n",
    "for i in range(20):\n",
    "    for j in range(120):\n",
    "        for k in range(480):\n",
    "            real_label[i][j][k]=label[i][j][k]\n",
    "real_label = np.array(real_label,dtype='float32')\n",
    "\n",
    "data_folder = '/home/hp209/Mocular/original/mat'\n",
    "files =os.listdir(data_folder)\n",
    "files.sort()\n",
    "data = []\n",
    "for file in files: \n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    #print(file_path)\n",
    "    a =loadmat(file_path)\n",
    "    a['D'] = np.array(a['D'],dtype = np.float32)\n",
    "    data.append(a['D'])    \n",
    "real_data = np.empty(shape=[20,352,800,3],dtype=np.float32)\n",
    "for i in range(20):\n",
    "    for j in range(352):\n",
    "        for k in range(800):\n",
    "            for s in range(3):\n",
    "                real_data[i][j][k][s]=data[i][j][k][s]\n",
    "real_data = np.array(real_data,dtype = 'float32')\n",
    "backward(real_data,real_label)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
